{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Points of interest based bicycle network generation\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the transit-oriented development approach of palominos2020ica or a grid approach and applies cardillo2006spp: Take the greedy triangulation between railway/underground stations (or other points of interest created in 02_prepare_pois). This is the cold start bicycle network generation process which creates bicycle networks from scratch.\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-06-18  \n",
    "Last modified: 2021-01-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug = True # If True, will produce plots and/or verbose output to double-check\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PATH.\n",
      "\n",
      "\n",
      "\n",
      "=== Cities ===\n",
      "{   'newcastle': {   'countryid': 'gbr',\n",
      "                     'name': 'Newcastle upon Tyne',\n",
      "                     'nominatimstring': 'Newcastle upon Tyne'}}\n",
      "==============\n",
      "\n",
      "\n",
      "Setup finished.\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.6\n",
      "IPython version      : 8.29.0\n",
      "\n",
      "Compiler    : MSC v.1941 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 875cd764df6d00c25afeeae14b25e4036eecf8c9\n",
      "\n",
      "geopandas : 0.14.4\n",
      "haversine : 2.8.1\n",
      "tqdm      : 4.66.5\n",
      "networkx  : 3.3\n",
      "shapely   : 2.0.6\n",
      "osgeo     : 3.9.3\n",
      "fiona     : 1.10.1\n",
      "rasterio  : 1.3.11\n",
      "owslib    : 0.32.0\n",
      "geojson   : 3.1.0\n",
      "numpy     : 1.26.4\n",
      "json      : 2.0.9\n",
      "watermark : 2.5.0\n",
      "sys       : 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:01:26) [MSC v.1941 64 bit (AMD64)]\n",
      "csv       : 1.0\n",
      "igraph    : 0.11.6\n",
      "matplotlib: 3.8.4\n",
      "pandas    : 2.2.3\n",
      "osmnx     : 1.9.4\n",
      "IPython   : 8.29.0\n",
      "pyproj    : 3.7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i path.py\n",
    "%run -i setup.py\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing (shortest paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and code below currently routes between the edges of neighbourhoods, rather than from a single point to a single point. We then join the neighbourhoods up first, before considering the wider area. This wider area is derived from hexagonal tesslleations within the city boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_triangulation_routing_mix(G, neighbourhood_nnids, tessellation_nnids, weighting=None, prune_quantiles=[1], prune_measure=\"betweenness\"):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DtypeWarning)\n",
    "\n",
    "    # Combine neighbourhood_nnids and tessellation_nnids with origin labels\n",
    "    pois = [(node_id, 'neighbourhood') for node_id in neighbourhood_nnids] + \\\n",
    "           [(node_id, 'tessellation') for node_id in tessellation_nnids]\n",
    "\n",
    "    if len(pois) < 2: \n",
    "        return [], []  # We can't do anything with fewer than 2 POIs\n",
    "\n",
    "    # Separate POIs by origin (neighbourhood and tessellation)\n",
    "    neighbourhood_pois = [poi for poi in pois if poi[1] == 'neighbourhood']\n",
    "    tessellation_pois = [poi for poi in pois if poi[1] == 'tessellation']\n",
    "\n",
    "    # Extract node IDs for use in GT creation\n",
    "    neighbourhood_ids = [poi[0] for poi in neighbourhood_pois]\n",
    "    tessellation_ids = [poi[0] for poi in tessellation_pois]\n",
    "\n",
    "    # Create POI pairs and indices for neighbourhoods first\n",
    "    pois_indices = [G.vs.find(id=poi[0]).index for poi in neighbourhood_pois]\n",
    "    poipairs = poipairs_by_distance(G, neighbourhood_ids, weighting, True)\n",
    "\n",
    "    if len(poipairs) == 0: \n",
    "        return [], []\n",
    "\n",
    "    # Handle edge order based on prune_measure\n",
    "    # create a random order for edges\n",
    "    if prune_measure == \"random\":\n",
    "        GT = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        for poipair, poipair_distance in poipairs:\n",
    "            poipair_ind = (GT.vs.find(id=poipair[0]).index, GT.vs.find(id=poipair[1]).index)\n",
    "            if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"],\n",
    "                                            GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "                GT.add_edge(poipair_ind[0], poipair_ind[1], weight=poipair_distance)\n",
    "        random.seed(0)\n",
    "        edgeorder = random.sample(range(GT.ecount()), k=GT.ecount())\n",
    "    else:\n",
    "        edgeorder = False\n",
    "\n",
    "    # Initialisation\n",
    "    GT_abstracts = []\n",
    "    neighbourhood_GT_abstracts = []\n",
    "    tessellation_GT_abstracts = []\n",
    "    GTs = []\n",
    "    all_shortest_paths = []\n",
    "    processed_pairs = set()\n",
    "    GT_indices = set()\n",
    "\n",
    "    # Greedy triangulation for neighbourhood POIs first\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation\", leave=False):\n",
    "        GT_abstract = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        neighbourhood_GT_abstracts.append(GT_abstract)\n",
    "\n",
    "        if debug:\n",
    "            temp = ig_to_geojson(GT_abstract)\n",
    "            temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "            temp_gdf = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "            ax = temp_gdf.plot(color=\"red\", label=f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Source Point\")\n",
    "            plt.title(f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Process tessellation POIs\n",
    "    pois_indices = [G.vs.find(id=poi[0]).index for poi in tessellation_pois]\n",
    "    poipairs = poipairs_by_distance(G, tessellation_ids, weighting, True)\n",
    "\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation for tessellation\", leave=False):\n",
    "        GT_abstract = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        tessellation_GT_abstracts.append(GT_abstract)\n",
    "\n",
    "        if debug:\n",
    "            temp = ig_to_geojson(GT_abstract)\n",
    "            temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "            temp_gdf2 = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "            ax = temp_gdf2.plot()\n",
    "            temp_gdf.plot(ax=ax, color=\"red\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "            plt.title(\"Abstract for tessellation\")\n",
    "            plt.show()\n",
    "\n",
    "    # Combine neighbourhood and tessellation graphs\n",
    "    GT_abstracts = neighbourhood_GT_abstracts + tessellation_GT_abstracts\n",
    "\n",
    "    # Assign source labels to edges in the combined abstracts\n",
    "    for GT_abstract in GT_abstracts:\n",
    "        source_value = \"neighbourhoods\" if GT_abstract in neighbourhood_GT_abstracts else \"tessellation\"\n",
    "        for edge in GT_abstract.es:\n",
    "            edge[\"source\"] = source_value\n",
    "\n",
    "    # Routing for GT_abstracts\n",
    "    GT_indices = set()\n",
    "    GT_abstracts_iterator = iter(GT_abstracts)\n",
    "\n",
    "    ## dounle the number of times we iterate as we have two input graphs rather than one\n",
    "    # note this may need changing\n",
    "    def double_prune_quantiles(prune_quantiles):\n",
    "        doubled_quantiles = []\n",
    "        for i in range(len(prune_quantiles) - 1):\n",
    "            doubled_quantiles.append(prune_quantiles[i])\n",
    "            doubled_quantiles.append((prune_quantiles[i] + prune_quantiles[i + 1]) / 2)\n",
    "        doubled_quantiles.append(prune_quantiles[-1])\n",
    "        return doubled_quantiles\n",
    "    dubs = double_prune_quantiles(prune_quantiles)\n",
    "\n",
    "    # do the routing\n",
    "    for prune_quantile in tqdm(dubs, desc=\"Routing\", leave=False):\n",
    "        GT_abstract = next(GT_abstracts_iterator)\n",
    "        routenodepairs = {(e.source_vertex[\"id\"], e.target_vertex[\"id\"]): e[\"weight\"] for e in GT_abstract.es}\n",
    "        routenodepairs = sorted(routenodepairs.items(), key=lambda x: x[1])\n",
    "\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G.vs.find(id=poipair[0]).index, G.vs.find(id=poipair[1]).index)\n",
    "            \n",
    "            # Neighbourhood routing logic\n",
    "            if any(poi[0] == poipair[0] and poi[1] == 'neighbourhood' for poi in pois):\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "                exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs: \n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        ea_vertex_index = G.vs.find(id=ea).index\n",
    "                        eb_vertex_index = G.vs.find(id=eb).index\n",
    "                        sp = G.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                        if sp and len(sp) < shortest_path_length:\n",
    "                            shortest_path_length, best_path = len(sp), sp\n",
    "\n",
    "                if best_path:\n",
    "                    all_shortest_paths.append((poipair[0], poipair[1], shortest_path_length, best_path))\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "                    GT = G.induced_subgraph([G.vs[idx] for idx in best_path])\n",
    "                    GTs.append(GT)\n",
    "            else:\n",
    "                # Routing for tessellation POIs\n",
    "                sp = set(G.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights=\"weight\", output=\"vpath\")[0])\n",
    "                GT_indices = GT_indices.union(sp)\n",
    "\n",
    "        # Final GT creation\n",
    "        GT = G.induced_subgraph(GT_indices)\n",
    "        GTs.append(GT)\n",
    "\n",
    "        if debug:\n",
    "            GT_gjson = ig_to_geojson(GT)\n",
    "            GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "            GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            GT_gdf.plot(ax=ax, color=\"blue\", linewidth=1.5, label=f\"GT for prune_quantile {prune_quantile}\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "            exit_points.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"All Exit Points\")\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Longitude\")\n",
    "            plt.ylabel(\"Latitude\")\n",
    "            plt.title(f\"Greedy Triangulation (GT) for prune_quantile = {prune_quantile}\")\n",
    "            plt.show()\n",
    "\n",
    "    return GTs, GT_abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4473d4fc8944678cb9daf89de3cd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cities:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newcastle: Loading location polygon and carall graph\n",
      "newcastle: Loading and moving POIs\n",
      "1 Cities loaded\n",
      "Processing neighbourhoods in: Newcastle Upon Tyne\n",
      "Loaded neighbourhoods and exit points for newcastle: Generating networks\n",
      "Loaded tessellation points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:448: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  e = pd.read_csv(p + prefix + '_edges.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9966f58df20b433dab18ba94d240e499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Greedy triangulation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a376d7e47fce46b281df629a9dd937c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Greedy triangulation for tessellation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceafcf96af9438b9dcb5915d255e3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Routing:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## do the triangulation and routing\n",
    "\n",
    "# Load data again to ensure we account for neighbourhoods correctly\n",
    "# Load all carall graphs in OSMNX format\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(f\"{placeid}: Loading location polygon and carall graph\")\n",
    "    \n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry']))  # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "    print(f\"{placeid}: Loading and moving POIs\")\n",
    "    # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and create GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  \n",
    "    exit_points = get_exit_nodes(neighbourhoods, G_carall)  # Requires osmnx G_carall, not igraph G_carall\n",
    "        \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    for name, gdf in neighbourhoods.items():  # Process each neighbourhood GeoDataFrame to get centroids, exit points, and neighbourhood IDs\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighbourhood in the GeoDataFrame to reference throughout\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        if debug:\n",
    "            print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "        unique_id += len(gdf)  # Increment by the number of neighbourhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    neighbourhood_nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in neighbourhood_nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            neighbourhood_nnids.add(n)\n",
    "    # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y))  # We now have all_centroids with 'neighbourhood_id', 'geometry', 'nearest_node' columns\n",
    "\n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidsbikeall.csv') as f:\n",
    "        tessellation_nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "    # Combine nnids\n",
    "    nnids = neighbourhood_nnids | set(tessellation_nnids)\n",
    "\n",
    "    # Load networks\n",
    "    # G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrackcarall', weighting=weighting)  # To load car and cycle network\n",
    "\n",
    "    # Generation\n",
    "    (GTs, GT_abstracts) = greedy_triangulation_routing_mix(G_carall, neighbourhood_nnids, tessellation_nnids, weighting, prune_quantiles, prune_measure)\n",
    "    (MST, MST_abstract) = mst_routing(G_carall, nnids, weighting)\n",
    "    \n",
    "    # Restore original edge lengths\n",
    "    if weighting:\n",
    "        restore_original_lengths(G_carall)\n",
    "        for GT in GTs:\n",
    "            restore_original_lengths(GT)\n",
    "        restore_original_lengths(MST)\n",
    "\n",
    "    # Write results\n",
    "    results = {\n",
    "        \"placeid\": placeid,\n",
    "        \"prune_measure\": prune_measure,\n",
    "        \"poi_source\": poi_source,\n",
    "        \"prune_quantiles\": prune_quantiles,\n",
    "        \"GTs\": GTs,\n",
    "        \"GT_abstracts\": GT_abstracts,\n",
    "        \"MST\": MST,\n",
    "        \"MST_abstract\": MST_abstract\n",
    "    }\n",
    "    write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poipairs_by_distance_mix(G, pois, weighting=None, return_distances=False):\n",
    "    \"\"\"\n",
    "    Calculates the (weighted) graph distances on G for a subset of nodes pois.\n",
    "    Returns all pairs of POI ids sorted by:\n",
    "    1. Source (neighbourhood-to-neighbourhood pairs first, then others).\n",
    "    2. Distance (ascending order).\n",
    "    If return_distances, then distances are also returned.\n",
    "    \"\"\"\n",
    "    # Get poi indices\n",
    "    indices = [G.vs.find(id=poi).index for poi in pois]\n",
    "\n",
    "    # Map POI ids to their source type (neighbourhood or tessellation)\n",
    "    poi_sources = {poi: 'neighbourhood' if poi in neighbourhood_nnids else 'tessellation' for poi in pois}\n",
    "\n",
    "    # Get sequences of nodes and edges in shortest paths between all pairs of pois\n",
    "    poi_nodes = []\n",
    "    poi_edges = []\n",
    "    for c, v in enumerate(indices):\n",
    "        poi_nodes.append(G.get_shortest_paths(v, indices[c:], weights=\"weight\", output=\"vpath\"))\n",
    "        poi_edges.append(G.get_shortest_paths(v, indices[c:], weights=\"weight\", output=\"epath\"))\n",
    "\n",
    "    # Sum up weights (distances) of all paths\n",
    "    poi_dist = {}\n",
    "    for paths_n, paths_e in zip(poi_nodes, poi_edges):\n",
    "        for path_n, path_e in zip(paths_n, paths_e):\n",
    "            if weighting:\n",
    "                path_dist = sum([G.es[e]['ori_length'] for e in path_e])  # Use 'ori_length' for distance\n",
    "            else:\n",
    "                path_dist = sum([G.es[e]['weight'] for e in path_e])  # Fallback to 'weight' if weighting is False\n",
    "\n",
    "            if path_dist > 0:\n",
    "                poi_dist[(path_n[0], path_n[-1])] = path_dist\n",
    "\n",
    "    # Back to POI ids and add source information\n",
    "    temp = [\n",
    "        ((G.vs[p[0][0]][\"id\"], G.vs[p[0][1]][\"id\"]), p[1],\n",
    "         poi_sources[G.vs[p[0][0]][\"id\"]], poi_sources[G.vs[p[0][1]][\"id\"]])\n",
    "        for p in poi_dist.items()\n",
    "    ]\n",
    "\n",
    "    # Sort by source (neighbourhood-to-neighbourhood first, then others), then by distance\n",
    "    temp_sorted = sorted(\n",
    "        temp,\n",
    "        key=lambda x: (\n",
    "            0 if x[2] == 'neighbourhood' and x[3] == 'neighbourhood' else 1,  # Primary: neighbourhood-to-neighbourhood first\n",
    "            x[1]  # Secondary: ascending distance\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Remove source information before returning (optional)\n",
    "    if return_distances:\n",
    "        return [(p[0], p[1]) for p in temp_sorted]\n",
    "    else:\n",
    "        return [p[0] for p in temp_sorted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### this try is to mix both into the same poi pairs\n",
    "\n",
    "\n",
    "def greedy_triangulation_routing_mix(G, neighbourhood_nnids, tessellation_nnids, weighting=None, prune_quantiles=[1], prune_measure=\"betweenness\"):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # Combine neighbourhood_nnids and tessellation_nnids with origin labels\n",
    "    pois = [(node_id, 'neighbourhood') for node_id in neighbourhood_nnids] + \\\n",
    "           [(node_id, 'tessellation') for node_id in tessellation_nnids]\n",
    "\n",
    "    if len(pois) < 2: \n",
    "        return [], []  # We can't do anything with less than 2 POIs\n",
    "\n",
    "    # Separate POIs by origin (neighbourhood and tessellation)\n",
    "    neighbourhood_pois = [poi for poi in pois if poi[1] == 'neighbourhood']\n",
    "    tessellation_pois = [poi for poi in pois if poi[1] == 'tessellation']\n",
    "\n",
    "    # Extract only the node ids for use in GT creation\n",
    "    neighbourhood_ids = [poi[0] for poi in neighbourhood_pois]\n",
    "    tessellation_ids = [poi[0] for poi in tessellation_pois]\n",
    "\n",
    "    # Combine all POI indices and create POI pairs for all POIs\n",
    "    combined_ids = neighbourhood_ids + tessellation_ids\n",
    "    pois_indices = [G.vs.find(id=poi).index for poi in combined_ids]\n",
    "    G_temp = copy.deepcopy(G)\n",
    "    for e in G_temp.es:  # Delete all edges\n",
    "        G_temp.es.delete(e)\n",
    "\n",
    "    # Use poipairs_by_distance for the combined set of points\n",
    "    poipairs = poipairs_by_distance_mix(G, combined_ids, weighting, True)\n",
    "\n",
    "    print(poipairs)\n",
    "\n",
    "\n",
    "    if len(poipairs) == 0: \n",
    "        return [], []\n",
    "\n",
    "    if prune_measure == \"random\":\n",
    "        # Create a random order for edges\n",
    "        GT = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        for poipair, poipair_distance in poipairs:\n",
    "            poipair_ind = (GT.vs.find(id=poipair[0]).index, GT.vs.find(id=poipair[1]).index)\n",
    "            if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"], GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "                GT.add_edge(poipair_ind[0], poipair_ind[1], weight=poipair_distance)\n",
    "        random.seed(0)\n",
    "        edgeorder = random.sample(range(GT.ecount()), k=GT.ecount())\n",
    "    else:\n",
    "        edgeorder = False\n",
    "\n",
    "    GT_abstracts = []\n",
    "    GTs = []\n",
    "    all_shortest_paths = []\n",
    "    processed_pairs = set()\n",
    "    GT_indices = set()  # Accumulated nodes for the final GT\n",
    "\n",
    "    # Greedy Triangulation for neighbourhood pois first\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation\", leave=False):\n",
    "        # GT creation for \"neighbourhood\" pois first\n",
    "        GT_abstract = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "        ############ Temporary code for debugging purposes ############\n",
    "        if debug:\n",
    "            temp = ig_to_geojson(GT_abstract)\n",
    "            temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "            temp_gdf = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "            print(\"Neighbourhood pois plot\")\n",
    "            ax = temp_gdf.plot(color = \"red\", label=f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Source Point\")\n",
    "            plt.title(f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            plt.show()   \n",
    "        ###############################################################     \n",
    "\n",
    "\n",
    "    ############ Temporary code for debugging purposes ############\n",
    "    if debug:\n",
    "        temp = ig_to_geojson(GT_abstract)\n",
    "        temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "        temp_gdf = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "        print(\"Neighbourhood pois plot\")\n",
    "        ax = temp_gdf.plot(color = \"red\")\n",
    "        all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Source Point\")\n",
    "        plt.title(\"GT_Abstract for neighbourhoods\")\n",
    "        plt.show()   \n",
    "    ###############################################################     \n",
    "\n",
    "\n",
    "    \n",
    "    if debug:\n",
    "        for edge in GT_abstracts[0].es:\n",
    "            print(edge.attributes())\n",
    "\n",
    "\n",
    "    # Routing and creating GT for each prune_quantile\n",
    "    GT_indices = set()\n",
    "\n",
    "    # set an interator to do processing per GT_abstract in GT_abstracts\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Routing\", leave=False):\n",
    "        # Update GT_abstract within each prune_quantile\n",
    "        GT_abstract = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "        \n",
    "        # get node pairs we need to route, sorted by distance\n",
    "        routenodepairs = { (e.source_vertex[\"id\"], e.target_vertex[\"id\"]): e[\"weight\"] for e in GT_abstract.es }\n",
    "        routenodepairs = sorted(routenodepairs.items(), key=lambda x: x[1])\n",
    "\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G.vs.find(id=poipair[0]).index, G.vs.find(id=poipair[1]).index)\n",
    "            \n",
    "            if any(poi[0] == poipair[0] and poi[1] == 'neighbourhood' for poi in pois):\n",
    "                # Neighbourhood routing logic\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "                exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs: \n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        ea_vertex_index = G.vs.find(id=ea).index\n",
    "                        eb_vertex_index = G.vs.find(id=eb).index\n",
    "                        sp = G.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                        if sp and len(sp) < shortest_path_length:\n",
    "                            shortest_path_length, best_path = len(sp), sp\n",
    "\n",
    "                if best_path:\n",
    "                    all_shortest_paths.append((poipair[0], poipair[1], shortest_path_length, best_path))\n",
    "                    GT_indices.update(best_path)\n",
    "                    \n",
    "                    # Plotting for neighbourhood routing\n",
    "                    GT = G.induced_subgraph([G.vs[idx] for idx in best_path])\n",
    "                    GTs.append(GT)\n",
    "            else:\n",
    "                # Routing for tessellation pois (more general case)\n",
    "                sp = set(G.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights=\"weight\", output=\"vpath\")[0])\n",
    "                GT_indices = GT_indices.union(sp)\n",
    "\n",
    "            \n",
    "        # Final step: Create the final GT from the indices we've accumulated\n",
    "        GT = G.induced_subgraph(GT_indices)\n",
    "        GTs.append(GT)\n",
    "\n",
    "        # Plot the current GT\n",
    "        GT_gjson = ig_to_geojson(GT)\n",
    "        GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "        GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "\n",
    "        # Plotting the GTs after each loop\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        GT_gdf.plot(ax=ax, color=\"blue\", linewidth=1.5, label=f\"GT for prune_quantile {prune_quantile}\")\n",
    "        \n",
    "        all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "        exit_points.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"All Exit Points\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.title(f\"Greedy Triangulation (GT) for prune_quantile = {prune_quantile}\")\n",
    "        plt.show()\n",
    "\n",
    "    return GTs, GT_abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS FUNCTION WORKS FOR ONE OR THE OTHER AT AT TIME\n",
    "\n",
    "\n",
    "def greedy_triangulation_routing_mix(G, neighbourhood_nnids, tessellation_nnids, weighting=None, prune_quantiles=[1], prune_measure=\"betweenness\"):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # Combine neighbourhood_nnids and tessellation_nnids with origin labels\n",
    "    pois = [(node_id, 'neighbourhood') for node_id in neighbourhood_nnids] + \\\n",
    "           [(node_id, 'tessellation') for node_id in tessellation_nnids]\n",
    "\n",
    "    if len(pois) < 2: \n",
    "        return [], []  # We can't do anything with less than 2 POIs\n",
    "\n",
    "    # Separate pois by origin (neighbourhood and tessellation)\n",
    "    neighbourhood_pois = [poi for poi in pois if poi[1] == 'neighbourhood']\n",
    "    tessellation_pois = [poi for poi in pois if poi[1] == 'tessellation']\n",
    "\n",
    "    # Extract only the node ids for use in GT creation\n",
    "    neighbourhood_ids = [poi[0] for poi in neighbourhood_pois]\n",
    "    tessellation_ids = [poi[0] for poi in tessellation_pois]\n",
    "\n",
    "    # Create POI pairs and indices for neighbourhoods first\n",
    "    pois_indices = [G.vs.find(id=poi[0]).index for poi in neighbourhood_pois]\n",
    "    poipairs = poipairs_by_distance(G, neighbourhood_ids, weighting, True)\n",
    "\n",
    "    if len(poipairs) == 0: \n",
    "        return [], []\n",
    "\n",
    "    if prune_measure == \"random\":\n",
    "        # Create a random order for edges\n",
    "        GT = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        for poipair, poipair_distance in poipairs:\n",
    "            poipair_ind = (GT.vs.find(id=poipair[0]).index, GT.vs.find(id=poipair[1]).index)\n",
    "            if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"], GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "                GT.add_edge(poipair_ind[0], poipair_ind[1], weight=poipair_distance)\n",
    "        random.seed(0)\n",
    "        edgeorder = random.sample(range(GT.ecount()), k=GT.ecount())\n",
    "    else:\n",
    "        edgeorder = False\n",
    "\n",
    "    GT_abstracts = []\n",
    "    neighbourhood_GT_abstracts = []\n",
    "    tessellation_GT_abstracts = []\n",
    "    GTs = []\n",
    "    all_shortest_paths = []\n",
    "    processed_pairs = set()\n",
    "    GT_indices = set()  # Accumulated nodes for the final GT\n",
    "\n",
    "    # Greedy Triangulation for neighbourhood pois first\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation\", leave=False):\n",
    "        # GT creation for \"neighbourhood\" pois first\n",
    "        GT_abstract = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        neighbourhood_GT_abstracts.append(GT_abstract)\n",
    "        ############ Temporary code for debugging purposes ############\n",
    "        if debug:\n",
    "            temp = ig_to_geojson(GT_abstract)\n",
    "            temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "            temp_gdf = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "            print(\"Neighbourhood pois plot\")\n",
    "            ax = temp_gdf.plot(color = \"red\", label=f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Source Point\")\n",
    "            plt.title(f\"GT_abstract for prune_quantile {prune_quantile}\")\n",
    "            plt.show()   \n",
    "        ###############################################################     \n",
    "\n",
    "\n",
    "    ############ Temporary code for debugging purposes ############\n",
    "    if debug:\n",
    "        temp = ig_to_geojson(GT_abstract)\n",
    "        temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "        temp_gdf = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "        print(\"Neighbourhood pois plot\")\n",
    "        ax = temp_gdf.plot(color = \"red\")\n",
    "        all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Source Point\")\n",
    "        plt.title(\"GT_Abstract for neighbourhoods\")\n",
    "        plt.show()   \n",
    "    ###############################################################     \n",
    "\n",
    "\n",
    "    # After processing all neighbourhood pois, now process tessellation pois\n",
    "    pois_indices = [G.vs.find(id=poi[0]).index for poi in tessellation_pois]\n",
    "    poipairs = poipairs_by_distance(G, tessellation_ids, weighting, True)\n",
    "\n",
    "    # GT creation for \"tessellation\" pois\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation for tessellation\", leave=False):\n",
    "        # Perform greedy triangulation for tessellation pois\n",
    "        GT_abstract = copy.deepcopy(G.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        tessellation_GT_abstracts.append(GT_abstract)\n",
    "\n",
    "\n",
    "\n",
    "    ############ Temporary code for debugging purposes ############\n",
    "    if debug:\n",
    "        temp = ig_to_geojson(GT_abstract)\n",
    "        temp_geo = [shape(geometry) for geometry in temp[\"geometries\"]]\n",
    "        temp_gdf2 = gpd.GeoDataFrame(geometry=temp_geo, crs=\"EPSG:4326\")\n",
    "        ax = temp_gdf2.plot()\n",
    "        temp_gdf.plot(ax=ax, color = \"red\")\n",
    "        all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "        plt.title(\"Abstract for tesselation\")\n",
    "        plt.show()\n",
    "    ###############################################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # note that this is two different abstract graphs mashed together, not one nice triangulated graph\n",
    "    # this may need changing, I am using to test the routing section   \n",
    "    GT_abstracts =  neighbourhood_GT_abstracts + tessellation_GT_abstracts \n",
    "    # to join two abstract graphs together, we ensure we can keep track of the source\n",
    "    for i, GT_abstract in enumerate(GT_abstracts):\n",
    "        # Determine the source type\n",
    "        if GT_abstract in neighbourhood_GT_abstracts:\n",
    "            source_value = \"neighbourhoods\"\n",
    "        elif GT_abstract in tessellation_GT_abstracts:\n",
    "            source_value = \"tessellation\"\n",
    "        else:\n",
    "            raise ValueError(\"Graph not found in either neighbourhood or tessellation lists!\")\n",
    "\n",
    "        # Update edges with the \"source\" attribute\n",
    "        for edge in GT_abstract.es:\n",
    "            edge[\"source\"] = source_value\n",
    "    \n",
    "    if debug:\n",
    "        for edge in GT_abstracts[0].es:\n",
    "            print(edge.attributes())\n",
    "\n",
    "\n",
    "    # Routing and creating GT for each prune_quantile\n",
    "    GT_indices = set()\n",
    "\n",
    "    # set an interator to do processing per GT_abstract in GT_abstracts\n",
    "    GT_abstracts_iterator = iter(GT_abstracts)\n",
    "\n",
    "    ##################################TEMPORARY CODE FOR DEBUGGING PURPOSES##################################\n",
    "    def double_prune_quantiles(prune_quantiles):\n",
    "        # Initialize an empty list to hold the new quantiles\n",
    "        doubled_quantiles = []\n",
    "        \n",
    "        # Iterate through the original quantiles and insert a midpoint between consecutive values\n",
    "        for i in range(len(prune_quantiles) - 1):\n",
    "            doubled_quantiles.append(prune_quantiles[i])  # Append the current quantile\n",
    "            # Calculate and append the midpoint between consecutive quantiles\n",
    "            midpoint = (prune_quantiles[i] + prune_quantiles[i + 1]) / 2\n",
    "            doubled_quantiles.append(midpoint)\n",
    "        \n",
    "        # Append the last element of the original list\n",
    "        doubled_quantiles.append(prune_quantiles[-1])\n",
    "        \n",
    "        return doubled_quantiles\n",
    "    ###########################################################################################################\n",
    "\n",
    "    \n",
    "    dubs = double_prune_quantiles(prune_quantiles)\n",
    "\n",
    "    for prune_quantile in tqdm(dubs, desc=\"Routing\", leave=False):\n",
    "        GT_abstract = next(GT_abstracts_iterator)\n",
    "        routenodepairs = { (e.source_vertex[\"id\"], e.target_vertex[\"id\"]): e[\"weight\"] for e in GT_abstract.es }\n",
    "        routenodepairs = sorted(routenodepairs.items(), key=lambda x: x[1])\n",
    "\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G.vs.find(id=poipair[0]).index, G.vs.find(id=poipair[1]).index)\n",
    "            \n",
    "            if any(poi[0] == poipair[0] and poi[1] == 'neighbourhood' for poi in pois):\n",
    "                # Neighbourhood routing logic\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "                exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs: \n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        ea_vertex_index = G.vs.find(id=ea).index\n",
    "                        eb_vertex_index = G.vs.find(id=eb).index\n",
    "                        sp = G.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                        if sp and len(sp) < shortest_path_length:\n",
    "                            shortest_path_length, best_path = len(sp), sp\n",
    "\n",
    "                if best_path:\n",
    "                    all_shortest_paths.append((poipair[0], poipair[1], shortest_path_length, best_path))\n",
    "                    GT_indices.update(best_path)\n",
    "                    \n",
    "                    # Plotting for neighbourhood routing\n",
    "                    GT = G.induced_subgraph([G.vs[idx] for idx in best_path])\n",
    "                    GTs.append(GT)\n",
    "            else:\n",
    "                # Routing for tessellation pois (more general case)\n",
    "                sp = set(G.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights=\"weight\", output=\"vpath\")[0])\n",
    "                GT_indices = GT_indices.union(sp)\n",
    "\n",
    "         \n",
    "        # Final step: Create the final GT from the indices we've accumulated\n",
    "        GT = G.induced_subgraph(GT_indices)\n",
    "        GTs.append(GT)\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            # Plot the current GT\n",
    "            GT_gjson = ig_to_geojson(GT)\n",
    "            GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "            GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "\n",
    "            # Plotting the GTs after each loop\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            GT_gdf.plot(ax=ax, color=\"blue\", linewidth=1.5, label=f\"GT for prune_quantile {prune_quantile}\")\n",
    "            \n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "            exit_points.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"All Exit Points\")\n",
    "\n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Longitude\")\n",
    "            plt.ylabel(\"Latitude\")\n",
    "            plt.title(f\"Greedy Triangulation (GT) for prune_quantile = {prune_quantile}\")\n",
    "            plt.show()\n",
    "\n",
    "    return GTs, GT_abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to load data again to ensure we take in neighbourhoods correctly\n",
    "# Load all carall graphs in OSMNX format\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Loading location polygon and carall graph\")\n",
    "    \n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry'])) # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "    print(placeid + \": loading and moving POIs\")\n",
    "    # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and create GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  \n",
    "    exit_points = get_exit_nodes(neighbourhoods, G_carall) # requires osmnx G_carall, not igraph G_carall\n",
    "        \n",
    "    \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    for name, gdf in neighbourhoods.items(): # Process each neighbourhood GeoDataFrame to get centroids, exit points, and neighbourhood IDs\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighborhood in the GeoDataFrame to referance throughout\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        if debug == True:\n",
    "            print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "        unique_id += len(gdf)  # Increment by the number of neighborhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    neighbourhood_nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in neighbourhood_nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            neighbourhood_nnids.add(n)\n",
    "        # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y)) # we now have all_centroids with 'neighbourhood_id', 'geometry', 'nearest_node' columns\n",
    "\n",
    "    # generate connections\n",
    "    print(\"Loaded neighbourhoods and exit points\", placeid + \": Generating networks\")\n",
    "\n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidsbikeall.csv') as f:\n",
    "        tessellation_nnids = [int(line.rstrip()) for line in f]\n",
    "    print(\"Loaded tesselation points\")\n",
    "\n",
    "    # combine nnids\n",
    "    nnids = neighbourhood_nnids | set(tessellation_nnids)\n",
    "\n",
    "    # Load networks\n",
    "    #G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrackcarall', weighting=weighting) # to load car and cycle network\n",
    "\n",
    "    # Generation\n",
    "    (GTs, GT_abstracts) = greedy_triangulation_routing_mix(G_carall, neighbourhood_nnids, tessellation_nnids, weighting, prune_quantiles, prune_measure)\n",
    "    (MST, MST_abstract) = mst_routing(G_carall, nnids, weighting)\n",
    "    \n",
    "    # Restore orignal edge lengths\n",
    "    if weighting == True:\n",
    "        restore_original_lengths(G_carall)\n",
    "        for GT in GTs:\n",
    "            restore_original_lengths(GT)\n",
    "        restore_original_lengths(MST)\n",
    "\n",
    "\n",
    "    # Write results\n",
    "    results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": prune_quantiles, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts, \"MST\": MST, \"MST_abstract\": MST_abstract}\n",
    "    write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tessellation_nnids)\n",
    "len(neighbourhood_nnids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Points of interest based bicycle network generation\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the transit-oriented development approach of palominos2020ica or a grid approach and applies cardillo2006spp: Take the greedy triangulation between railway/underground stations (or other points of interest created in 02_prepare_pois). This is the cold start bicycle network generation process which creates bicycle networks from scratch.\n",
    "\n",
    "Contact: Michael Szell (michael.szell@gmail.com)  \n",
    "Created: 2020-06-18  \n",
    "Last modified: 2021-01-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PATH.\n",
      "\n",
      "Setup finished.\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.6\n",
      "IPython version      : 8.27.0\n",
      "\n",
      "Compiler    : MSC v.1941 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 140 Stepping 1, GenuineIntel\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 040e0b6343eebfb26be63f2865905d4258eec91e\n",
      "\n",
      "pandas    : 2.2.3\n",
      "osmnx     : 1.9.4\n",
      "json      : 2.0.9\n",
      "csv       : 1.0\n",
      "igraph    : 0.11.6\n",
      "watermark : 2.5.0\n",
      "tqdm      : 4.66.5\n",
      "geopandas : 0.14.4\n",
      "geojson   : 3.1.0\n",
      "IPython   : 8.27.0\n",
      "fiona     : 1.10.1\n",
      "matplotlib: 3.8.4\n",
      "haversine : 2.8.1\n",
      "pyproj    : 3.6.1\n",
      "sys       : 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:01:26) [MSC v.1941 64 bit (AMD64)]\n",
      "numpy     : 1.26.4\n",
      "shapely   : 2.0.6\n",
      "osgeo     : 3.9.2\n",
      "networkx  : 3.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i path.py\n",
    "%run -i setup.py\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded functions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing (shortest paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no longer needed\n",
    "# modded function 22:10 29/09/2024\n",
    "def greedy_triangulation_routing_neighbourhoods(G, pois, weighting=None, prune_quantiles = [1], prune_measure = \"betweenness\"):\n",
    "    \"\"\"Greedy Triangulation (GT) of a graph G's node subset pois,\n",
    "    then routing to connect the GT (up to a quantile of betweenness\n",
    "    betweenness_quantile).\n",
    "    G is an ipgraph graph, pois is a list of node ids.\n",
    "    \n",
    "    The GT connects pairs of nodes in ascending order of their distance provided\n",
    "    that no edge crossing is introduced. It leads to a maximal connected planar\n",
    "    graph, while minimizing the total length of edges considered. \n",
    "    See: cardillo2006spp\n",
    "    \n",
    "    Distance here is routing distance, while edge crossing is checked on an abstract \n",
    "    level.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pois) < 2: return ([], []) # We can't do anything with less than 2 POIs\n",
    "\n",
    "    # GT_abstract is the GT with same nodes but euclidian links to keep track of edge crossings\n",
    "    pois_indices = set()\n",
    "    for poi in pois:\n",
    "        pois_indices.add(G.vs.find(id = poi).index)\n",
    "    G_temp = copy.deepcopy(G)\n",
    "    for e in G_temp.es: # delete all edges\n",
    "        G_temp.es.delete(e)\n",
    "        \n",
    "    poipairs = poipairs_by_distance(G, pois, weighting, True)\n",
    "    if len(poipairs) == 0: return ([], [])\n",
    "\n",
    "    if prune_measure == \"random\":\n",
    "        # run the whole GT first\n",
    "        GT = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        for poipair, poipair_distance in poipairs:\n",
    "            poipair_ind = (GT.vs.find(id = poipair[0]).index, GT.vs.find(id = poipair[1]).index)\n",
    "            if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"], GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "                GT.add_edge(poipair_ind[0], poipair_ind[1], weight = poipair_distance)\n",
    "        # create a random order for the edges\n",
    "        random.seed(0) # const seed for reproducibility\n",
    "        edgeorder = random.sample(range(GT.ecount()), k = GT.ecount())\n",
    "    else: \n",
    "        edgeorder = False\n",
    "    \n",
    "    GT_abstracts = []\n",
    "    GTs = []\n",
    "    all_shortest_paths = []\n",
    "    # Track processed exit point pairs to avoid redundant routing\n",
    "    processed_pairs = set()\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc = \"Greedy triangulation\", leave = False):\n",
    "        GT_abstract = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "        \n",
    "        # Get node pairs we need to route, sorted by distance\n",
    "        routenodepairs = {}\n",
    "        for e in GT_abstract.es:\n",
    "            routenodepairs[(e.source_vertex[\"id\"], e.target_vertex[\"id\"])] = e[\"weight\"]\n",
    "        routenodepairs = sorted(routenodepairs.items(), key = lambda x: x[1])\n",
    "        #print(routenodepairs)\n",
    "\n",
    "        # Do the routing\n",
    "        GT_indices = set()\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            print(\"Routing between:\", poipair, \"with\", poipair_distance)\n",
    "            poipair_ind = (G.vs.find(id = poipair[0]).index, G.vs.find(id = poipair[1]).index)\n",
    "         \n",
    "            # debug\n",
    "            #print(f\"Edge weights before routing: {G.es['weight'][:10]}\")  # Prints first 10 weights\n",
    "            #print(f\"Routing between: {poipair[0]} and {poipair[1]} with distance: {poipair_distance}\")\n",
    "            ##sp = set(G.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights = \"weight\", output = \"vpath\")[0])\n",
    "            # Identify neighborhoods for each POI and retrieve their exit points\n",
    "            #print(all_centroids['nearest_node'])\n",
    "            #print(poipair[0])\n",
    "            #print(poipair[1])\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "            exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "            shortest_path_length, best_path, edges_in_best_path = float('inf'), None, []\n",
    "            # Compute shortest paths between exit point pairs\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs: \n",
    "                        continue  # Skip if already processed\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    ea_vertex_index = G.vs.find(id=ea).index\n",
    "                    eb_vertex_index = G.vs.find(id=eb).index\n",
    "                    sp = G.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                    # Update if a shorter path is found\n",
    "                    if sp:\n",
    "                        path_length = len(sp)\n",
    "                        if path_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = path_length, sp\n",
    "                            edges_in_best_path = [(sp[i], sp[i + 1]) for i in range(len(sp) - 1)]\n",
    "\n",
    "            # Store the results if a path was found\n",
    "            if shortest_path_length < float('inf'):\n",
    "                #print(f\"Shortest path between {node_a} and {node_b} is {shortest_path_length}\")\n",
    "                all_shortest_paths.append((poipair[0], poipair[1], shortest_path_length, best_path))\n",
    "                # Create subgraphs for GT \n",
    "                GT = G.induced_subgraph([G.vs[idx] for idx in best_path])\n",
    "                GTs.append(GT)\n",
    "                print(\"Stored GT Between\", poipair[0], \"and\", poipair[1], \"with best path\", best_path)\n",
    "            else:\n",
    "                print(f\"No path found between {poipair[0]} and {poipair[1]}\")\n",
    "            #print(f\"Shortest path between {poipair[0]} and {poipair[1]}: {sp}\")\n",
    "\n",
    "            GT_indices = GT_indices.union(sp)\n",
    "\n",
    "        GT = G.induced_subgraph(GT_indices)\n",
    "        GTs.append(GT)\n",
    "    \n",
    "    return (GTs, GT_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below does the same as the modded code but with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_triangulation_routing_neighbourhoods(G, pois, weighting=None, prune_quantiles=[1], prune_measure=\"betweenness\"):\n",
    "    \"\"\"Greedy Triangulation (GT) of a graph G's node subset pois,\n",
    "    then routing to connect the GT (up to a quantile of betweenness\n",
    "    betweenness_quantile).\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(pois) < 2: \n",
    "        return ([], [])  # We can't do anything with less than 2 POIs\n",
    "\n",
    "    # Initialise structures\n",
    "    pois_indices = set(G.vs.find(id=poi).index for poi in pois)\n",
    "    G_temp = copy.deepcopy(G)\n",
    "    for e in G_temp.es:  # Delete all edges\n",
    "        G_temp.es.delete(e)\n",
    "        \n",
    "    poipairs = poipairs_by_distance(G, pois, weighting, True)\n",
    "    if len(poipairs) == 0: \n",
    "        return ([], [])\n",
    "\n",
    "    if prune_measure == \"random\":\n",
    "        # Create a random order for edges\n",
    "        GT = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        for poipair, poipair_distance in poipairs:\n",
    "            poipair_ind = (GT.vs.find(id=poipair[0]).index, GT.vs.find(id=poipair[1]).index)\n",
    "            if not new_edge_intersects(GT, (GT.vs[poipair_ind[0]][\"x\"], GT.vs[poipair_ind[0]][\"y\"], GT.vs[poipair_ind[1]][\"x\"], GT.vs[poipair_ind[1]][\"y\"])):\n",
    "                GT.add_edge(poipair_ind[0], poipair_ind[1], weight=poipair_distance)\n",
    "        random.seed(0)\n",
    "        edgeorder = random.sample(range(GT.ecount()), k=GT.ecount())\n",
    "    else: \n",
    "        edgeorder = False\n",
    "    \n",
    "    \n",
    "    GT_abstracts = []\n",
    "    GTs = []\n",
    "    all_shortest_paths = []\n",
    "    processed_pairs = set()\n",
    "    GT_indices = set()  # Accumulated nodes for the final GT\n",
    "\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation\", leave=False):\n",
    "        # Update GT_abstract within each prune_quantile\n",
    "        GT_abstract = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(GT_abstract, poipairs, prune_quantile, prune_measure, edgeorder)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "        \n",
    "        routenodepairs = { (e.source_vertex[\"id\"], e.target_vertex[\"id\"]): e[\"weight\"] for e in GT_abstract.es }\n",
    "        routenodepairs = sorted(routenodepairs.items(), key=lambda x: x[1])\n",
    "\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G.vs.find(id=poipair[0]).index, G.vs.find(id=poipair[1]).index)\n",
    "\n",
    "            # Compute the shortest path\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "            exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs: \n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    ea_vertex_index = G.vs.find(id=ea).index\n",
    "                    eb_vertex_index = G.vs.find(id=eb).index\n",
    "                    sp = G.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                    if sp and len(sp) < shortest_path_length:\n",
    "                        shortest_path_length, best_path = len(sp), sp\n",
    "\n",
    "            # Accumulate nodes for final GT from best_path\n",
    "            if best_path:\n",
    "                all_shortest_paths.append((poipair[0], poipair[1], shortest_path_length, best_path))\n",
    "                GT_indices.update(best_path)  # Add nodes to GT_indices\n",
    "\n",
    "                # Plotting of current GT for each best_path found\n",
    "                GT = G.induced_subgraph([G.vs[idx] for idx in best_path])\n",
    "                GT_gjson = ig_to_geojson(GT)\n",
    "                GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "                \n",
    "                if GT_geometries:\n",
    "                    GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "\n",
    "                    if debug:\n",
    "                        # Plot the GT with neighborhood points and exit points\n",
    "                        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "                        GT_gdf.plot(ax=ax, color=\"blue\", linewidth=1, label=\"GT Edges\")\n",
    "                        neighbourhood_gdf = gpd.GeoDataFrame(\n",
    "                            geometry=[all_centroids.loc[all_centroids['nearest_node'] == poipair[0], 'geometry'].values[0], \n",
    "                                    all_centroids.loc[all_centroids['nearest_node'] == poipair[1], 'geometry'].values[0]], \n",
    "                            crs=\"EPSG:4326\"\n",
    "                        )\n",
    "                        neighbourhood_gdf.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"Neighbourhood Points\")\n",
    "                        exit_points_gdf = gpd.GeoDataFrame(\n",
    "                            geometry=list(exit_points.loc[exit_points['neighbourhood_id'].isin([neighbourhood_a, neighbourhood_b]), 'geometry']),\n",
    "                            crs=\"EPSG:4326\"\n",
    "                        )\n",
    "                        exit_points_gdf.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"Exit Points\")\n",
    "\n",
    "                        plt.legend()\n",
    "                        plt.xlabel(\"Longitude\")\n",
    "                        plt.ylabel(\"Latitude\")\n",
    "                        plt.title(f\"GT with Neighbourhood and Exit Points for POI Pair {poipair}\")\n",
    "                        plt.show()\n",
    "        \n",
    "        # Plot cumulative GTs up to the current prune_quantile\n",
    "        GT = G.induced_subgraph(GT_indices)  # Cumulative GT for current prune_quantile\n",
    "        GTs.append(GT)\n",
    "\n",
    "        if debug:\n",
    "            # Plot GTs after each prune_quantile loop\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            GT_gjson = ig_to_geojson(GT)\n",
    "            GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "            GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "            \n",
    "            GT_gdf.plot(ax=ax, color=\"blue\", linewidth=1.5, label=\"GT Edges\")\n",
    "            all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "            exit_points.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"Exit Points\")\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.xlabel(\"Longitude\")\n",
    "            plt.ylabel(\"Latitude\")\n",
    "            plt.title(\"GTs After Adding GT for POI Pair Loop\")\n",
    "            plt.show()\n",
    "\n",
    "    if debug:\n",
    "        # Final cumulative plot with all GTs, all centroids, and exit points, and G as background\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        G_gjson = ig_to_geojson(G)\n",
    "        G_geometries = [shape(geometry) for geometry in G_gjson[\"geometries\"]]\n",
    "        G_gdf = gpd.GeoDataFrame(geometry=G_geometries, crs=\"EPSG:4326\")\n",
    "        G_gdf.plot(ax=ax, color=\"lightgray\", linewidth=0.5, label=\"Full Network (G)\")\n",
    "\n",
    "        # Plot each GT with a unique color\n",
    "        for idx, GT in enumerate(GTs):\n",
    "            GT_gjson = ig_to_geojson(GT)\n",
    "            GT_geometries = [shape(geometry) for geometry in GT_gjson[\"geometries\"]]\n",
    "            \n",
    "            if GT_geometries:\n",
    "                GT_gdf = gpd.GeoDataFrame(geometry=GT_geometries, crs=\"EPSG:4326\")\n",
    "                GT_gdf.plot(ax=ax, color=plt.cm.tab10(idx % 10), linewidth=1.5, label=f\"GT {idx+1}\")\n",
    "\n",
    "        all_centroids.plot(ax=ax, color=\"green\", marker=\"o\", markersize=50, label=\"All Centroids\")\n",
    "        exit_points.plot(ax=ax, color=\"red\", marker=\"x\", markersize=30, label=\"All Exit Points\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.title(\"All GTs After Processing with Full Network, Centroids, and Exit Points\")\n",
    "        plt.show()\n",
    "\n",
    "    return (GTs, GT_abstracts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cities:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newcastle: Loading location polygon and carall graph\n",
      "newcastle: loading and moving POIs\n",
      "1 Cities loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1926: FutureWarning: Currently, index_parts defaults to True, but in the future, it will default to False to be consistent with Pandas. Use `index_parts=True` to keep the current behavior and True/False to silence the warning.\n",
      "  exploded_gdf = gdf.explode().reset_index(drop=True)\n",
      "c:\\Users\\b8008458\\AppData\\Local\\miniforge3\\envs\\growbikenet\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1730: FutureWarning: The `north`, `south`, `east`, and `west` parameters are deprecated and will be removed in the v2.0.0 release. Use the `bbox` parameter instead. See the OSMnx v2 migration guide: https://github.com/gboeing/osmnx/issues/1123\n",
      "  network = ox.graph_from_bbox(maxy, miny, maxx, minx, network_type='all')\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1730: FutureWarning: The expected order of coordinates in `bbox` will change in the v2.0.0 release to `(left, bottom, right, top)`.\n",
      "  network = ox.graph_from_bbox(maxy, miny, maxx, minx, network_type='all')\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1824: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "  if row.geometry.type == 'LineString':\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1826: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "  elif row.geometry.type == 'MultiLineString':\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1841: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "  if row.geometry.type == 'LineString':\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1845: ShapelyDeprecationWarning: The 'type' attribute is deprecated, and will be removed in the future. You can use the 'geom_type' attribute instead.\n",
      "  elif row.geometry.type == 'MultiLineString':\n",
      "C:\\Users\\b8008458\\AppData\\Local\\Temp\\ipykernel_16316\\2241757991.py:40: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exit_points = get_exit_nodes(neighbourhoods, G_carall) # requires osmnx G_carall, not igraph G_carall\n",
      "c:\\Users\\b8008458\\AppData\\Local\\miniforge3\\envs\\growbikenet\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:1666: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = gdf.geometry.centroid  # Calculate centroids for each polygon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing neighbourhoods in: Newcastle Upon Tyne\n",
      "newcastle: Generating networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth\\code\\functions.py:462: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  e['maxspeed'].fillna(20, inplace=True)  # Assign default speed of 20 where NaN\n",
      "Cities: 100%|██████████| 1/1 [02:41<00:00, 161.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# modded code 22:06 29/10/2024\n",
    "######### THIS SECTION LOADS THE DATA WE NEED #########\n",
    "# Load all carall graphs in OSMNX format\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Loading location polygon and carall graph\")\n",
    "    \n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry'])) # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "    print(placeid + \": loading and moving POIs\")\n",
    "    # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and create GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  \n",
    "    exit_points = get_exit_nodes(neighbourhoods, G_carall) # requires osmnx G_carall, not igraph G_carall\n",
    "        \n",
    "    \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    for name, gdf in neighbourhoods.items(): # Process each neighbourhood GeoDataFrame to get centroids, exit points, and neighbourhood IDs\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighborhood in the GeoDataFrame to referance throughout\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        if debug == True:\n",
    "            print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "        unique_id += len(gdf)  # Increment by the number of neighborhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            nnids.add(n)\n",
    "        # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "    )\n",
    "    # we now have all_centroids with 'neighbourhood_id', 'geometry', 'nearest_node' columns\n",
    "\n",
    "    # generate connections\n",
    "    print(placeid + \": Generating networks\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "    # Generation\n",
    "    (GTs, GT_abstracts) = greedy_triangulation_routing_neighbourhoods(G_carall, nnids, weighting, prune_quantiles, prune_measure)\n",
    "    (MST, MST_abstract) = mst_routing(G_carall, nnids, weighting)\n",
    "    \n",
    "    # Restore orignal edge lengths\n",
    "    if weighting == True:\n",
    "        restore_original_lengths(G_carall)\n",
    "        for GT in GTs:\n",
    "            restore_original_lengths(GT)\n",
    "        restore_original_lengths(MST)\n",
    "\n",
    "\n",
    "    # Write results\n",
    "    results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": prune_quantiles, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts, \"MST\": MST, \"MST_abstract\": MST_abstract}\n",
    "    write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### THIS SECTION LOADS THE DATA WE NEED #########\n",
    "# Load all carall graphs in OSMNX format\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Loading location polygon and carall graph\")\n",
    "    \n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry'])) # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(placeid + \": loading and moving POIs\")\n",
    "\n",
    "    # We need the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "   # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and initialize GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  # Adjust the CRS if necessary\n",
    "    \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    # Process each neighbourhood GeoDataFrame\n",
    "    for name, gdf in neighbourhoods.items():\n",
    "        # Check if gdf is empty\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighborhood in the GeoDataFrame\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        \n",
    "        # Print to confirm ID assignment\n",
    "        print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids and ensure to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "\n",
    "        # Append the centroids to all_centroids GeoDataFrame\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "\n",
    "        # Update unique_id for the next set of neighborhoods\n",
    "        unique_id += len(gdf)  # Increment by the number of neighborhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            nnids.add(n)\n",
    "        # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "    )\n",
    "\n",
    "\n",
    "### THIS SECTION OF CODE IS FOR ROUTING BETWEEN EXIT POINTS ###\n",
    "\n",
    "# Load neighbourhoods and convert graph to GeoDataFrames\n",
    "neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "exit_points = get_exit_nodes(neighbourhoods, G_carall) # requires osmnx G_carall, not igraph G_carall\n",
    "print(\"Got neigbourhoods, exit points, and centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### THIS SECTION LOADS THE DATA WE NEED #########\n",
    "# Load all carall graphs in OSMNX format\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc = \"Cities\"):\n",
    "    print(placeid + \": Loading location polygon and carall graph\")\n",
    "    \n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry'])) # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326' # needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(placeid + \": loading and moving POIs\")\n",
    "\n",
    "    # We need the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "   # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and initialize GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  # Adjust the CRS if necessary\n",
    "    \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    # Process each neighbourhood GeoDataFrame\n",
    "    for name, gdf in neighbourhoods.items():\n",
    "        # Check if gdf is empty\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighborhood in the GeoDataFrame\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        \n",
    "        # Print to confirm ID assignment\n",
    "        print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids and ensure to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "\n",
    "        # Append the centroids to all_centroids GeoDataFrame\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "\n",
    "        # Update unique_id for the next set of neighborhoods\n",
    "        unique_id += len(gdf)  # Increment by the number of neighborhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            nnids.add(n)\n",
    "        # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "    )\n",
    "\n",
    "\n",
    "### THIS SECTION OF CODE IS FOR ROUTING BETWEEN EXIT POINTS ###\n",
    "\n",
    "# Load neighbourhoods and convert graph to GeoDataFrames\n",
    "neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "exit_points = get_exit_nodes(neighbourhoods, G_carall) # requires osmnx G_carall, not igraph G_carall\n",
    "print(\"Got neigbourhoods, exit points, and centroids\")\n",
    "\n",
    "# Load networks\n",
    "G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "\n",
    "all_shortest_paths = []\n",
    "GTs = []\n",
    "GT_abstracts = []\n",
    "processed_pairs = set()  # To track processed exit point pairs\n",
    "\n",
    "\n",
    "\n",
    "print(\"Beginning routing between exit points\")\n",
    "# Get points based on neighbourhood centroids\n",
    "pois = all_centroids['nearest_node'].tolist()\n",
    "poipairs = poipairs_by_distance(G_carall, pois, weighting, return_distances=True)\n",
    "\n",
    "# Track processed exit point pairs to avoid redundant routing\n",
    "processed_pairs = set()\n",
    "\n",
    "for prune_quantiles in tqdm(prune_quantiles, desc = \"Greedy Triangulation with neighbourhoods\", leave = False):\n",
    "    # Process each POI pair from poipairs\n",
    "    for (node_a, node_b), distance in tqdm(poipairs):\n",
    "        print(\"Processing POI pair:\", node_a, node_b)\n",
    "        # Identify neighborhoods for each POI and retrieve their exit points\n",
    "        neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == node_a, 'neighbourhood_id'].values[0]\n",
    "        neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == node_b, 'neighbourhood_id'].values[0]\n",
    "        exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "        exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "        shortest_path_length, best_path, edges_in_best_path = float('inf'), None, []\n",
    "        # Compute shortest paths between exit point pairs\n",
    "        for ea in exit_points_a:\n",
    "            for eb in exit_points_b:\n",
    "                pair_id = tuple(sorted((ea, eb)))\n",
    "                if pair_id in processed_pairs: \n",
    "                    continue  # Skip if already processed\n",
    "                processed_pairs.add(pair_id)\n",
    "                ea_vertex_index = G_carall.vs.find(id=ea).index\n",
    "                eb_vertex_index = G_carall.vs.find(id=eb).index\n",
    "                sp = G_carall.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                # Update if a shorter path is found\n",
    "                if sp:\n",
    "                    path_length = len(sp)\n",
    "                    if path_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = path_length, sp\n",
    "                        edges_in_best_path = [(sp[i], sp[i + 1]) for i in range(len(sp) - 1)]\n",
    "\n",
    "        # Store the results if a path was found\n",
    "        if shortest_path_length < float('inf'):\n",
    "            #print(f\"Shortest path between {node_a} and {node_b} is {shortest_path_length}\")\n",
    "            all_shortest_paths.append((node_a, node_b, shortest_path_length, best_path))\n",
    "            # Create subgraphs for GT \n",
    "            GT = G_carall.induced_subgraph([G_carall.vs[idx] for idx in best_path])\n",
    "            GTs.append(GT)\n",
    "            print(\"Stored GT\")\n",
    "        else:\n",
    "            print(f\"No path found between {node_a} and {node_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempt 4\n",
    "# Load graph and set CRS if not present\n",
    "G_carall = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "if 'crs' not in G_carall.graph:\n",
    "    G_carall.graph['crs'] = 'epsg:4326'\n",
    "\n",
    "# Load neighbourhoods and convert graph to GeoDataFrames\n",
    "neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "exit_points = get_exit_nodes(neighbourhoods, G_carall)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# edges = ox.graph_to_gdfs(G_carall, nodes=False, edges=True)\n",
    "# nodes = ox.graph_to_gdfs(G_carall, nodes=True, edges=False)\n",
    "\n",
    "# # Add unique IDs to each polygon in neighbourhoods and buffer them\n",
    "# boundary_buffers = {}\n",
    "# for place_name, gdf in neighbourhoods.items():\n",
    "#     exploded_gdf = gdf.explode().reset_index(drop=True)\n",
    "#     exploded_gdf['neighbourhood_id'] = exploded_gdf.index  # Unique ID for each polygon\n",
    "#     buffer = exploded_gdf.boundary.to_crs(epsg=3857).buffer(10).to_crs(exploded_gdf.crs)  # 10-meter buffer\n",
    "#     boundary_buffers[place_name] = (buffer, exploded_gdf)\n",
    "\n",
    "# # Combine all buffers into a single GeoDataFrame and set the geometry\n",
    "# buffer_geometries = [boundary_buffers[place][0] for place in boundary_buffers]\n",
    "# neighbourhood_geometries = [boundary_buffers[place][1]['neighbourhood_id'] for place in boundary_buffers]\n",
    "\n",
    "# # Create a GeoDataFrame from the geometries\n",
    "# boundary_buffers_gdf = gpd.GeoDataFrame(geometry=pd.concat(buffer_geometries, ignore_index=True))\n",
    "# boundary_buffers_gdf['neighbourhood_id'] = pd.concat(neighbourhood_geometries, ignore_index=True)\n",
    "\n",
    "# # Ensure CRS is set correctly\n",
    "# nodes_within_buffer = gpd.sjoin(nodes, boundary_buffers_gdf, how='inner', op='intersects')\n",
    "# #nodes_within_buffer = nodes[nodes.geometry.apply(lambda point: buffer.contains(point).any())]\n",
    "\n",
    "# street_buffers = {}\n",
    "# for place_name, gdf in neighbourhoods.items():\n",
    "#     street_nodes, street_edges = get_neighbourhood_streets_split(gdf, debug=False)\n",
    "#     # Buffering edges with correct CRS\n",
    "#     street_buffer = street_edges.to_crs(epsg=3857).geometry.buffer(100).to_crs(street_edges.crs)\n",
    "#     street_buffers[place_name] = gpd.GeoDataFrame(geometry=street_buffer, crs=street_edges.crs)\n",
    "\n",
    "\n",
    "# streets_buffer_gdf = gpd.GeoDataFrame(\n",
    "#     pd.concat([gdf for gdf in street_buffers.values()]), \n",
    "#     crs=next(iter(street_buffers.values())).crs\n",
    "# )\n",
    "\n",
    "\n",
    "# # Drop the 'index_right' column to avoid conflict (fix later)\n",
    "# if 'index_right' in nodes_within_buffer.columns:\n",
    "#     nodes_within_buffer = nodes_within_buffer.drop(columns=['index_right'])\n",
    "\n",
    "# nodes_within_buffer = gpd.sjoin(nodes_within_buffer, streets_buffer_gdf, how='inner', op='intersects')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize a dictionary to hold GeoDataFrames of nodes within each buffered boundary\n",
    "# boundary_nodes = {place_name: gpd.GeoDataFrame(columns=nodes.columns) for place_name in buffered_boundaries.keys()}\n",
    "\n",
    "# # Find nodes within buffered boundaries\n",
    "# for place_name, (buffer, gdf) in buffered_boundaries.items():\n",
    "#     intersecting_nodes = []\n",
    "#     for idx, node in nodes.iterrows():\n",
    "#         # Check intersection with the buffered boundary\n",
    "#         if buffer.intersects(node.geometry).any():\n",
    "#             # Get the unique neighbourhood_id from the corresponding polygon\n",
    "#             for poly_idx, poly in gdf.iterrows():\n",
    "#                 if poly.geometry.intersects(node.geometry):  # Check if the node intersects the polygon\n",
    "#                     node_with_id = node.copy()  # Create a copy of the node\n",
    "#                     node_with_id['neighbourhood_id'] = poly['neighbourhood_id']  # Assign neighbourhood ID from the polygon\n",
    "#                     intersecting_nodes.append(node_with_id)\n",
    "\n",
    "#     # Create a GeoDataFrame from the list of intersecting nodes\n",
    "#     if intersecting_nodes:  # Check if there are any intersecting nodes\n",
    "#         concat_df = pd.concat(intersecting_nodes, axis=1).T  # Transpose to get rows as nodes\n",
    "#         boundary_nodes[place_name] = gpd.GeoDataFrame(concat_df, geometry='geometry', crs=nodes.crs)\n",
    "\n",
    "# # Combine all GeoDataFrames into one for exploration\n",
    "# combined_boundary_nodes = gpd.GeoDataFrame(pd.concat([gdf for gdf in boundary_nodes.values() if not gdf.empty]), crs=nodes.crs)\n",
    "\n",
    "# # Use the .explore() method to visualize the combined GeoDataFrame\n",
    "# combined_boundary_nodes.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION OF CODE IS FOR ROUTING BETWEEN EXIT POINTS ###\n",
    "\n",
    "# Initialize list to store shortest paths for each POI pair\n",
    "all_shortest_paths = []\n",
    "\n",
    "# Extract nearest nodes and POI pairs based on distances\n",
    "pois = all_centroids['nearest_node'].tolist()\n",
    "poipairs = poipairs_by_distance(G_carall, pois, weighting, return_distances=True)\n",
    "\n",
    "# Output lists for GTs and GT_abstracts\n",
    "GTs = []\n",
    "\n",
    "# Track processed exit point pairs to avoid redundant routing\n",
    "processed_pairs = set()\n",
    "\n",
    "# Process each POI pair from poipairs\n",
    "for (node_a, node_b), distance in tqdm(poipairs, desc=\"Processing POI pairs\"):\n",
    "    # Identify neighborhoods for each POI and retrieve their exit points\n",
    "    neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == node_a, 'neighbourhood_id'].values[0]\n",
    "    neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == node_b, 'neighbourhood_id'].values[0]\n",
    "    exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "    exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "    shortest_path_length, best_path, edges_in_best_path = float('inf'), None, []\n",
    "    # Compute shortest paths between exit point pairs\n",
    "    for ea in exit_points_a:\n",
    "        for eb in exit_points_b:\n",
    "            pair_id = tuple(sorted((ea, eb)))\n",
    "            if pair_id in processed_pairs: \n",
    "                continue  # Skip if already processed\n",
    "            processed_pairs.add(pair_id)\n",
    "            ea_vertex_index = G_carall.vs.find(id=ea).index\n",
    "            eb_vertex_index = G_carall.vs.find(id=eb).index\n",
    "            sp = G_carall.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "            # Update if a shorter path is found\n",
    "            if sp:\n",
    "                path_length = len(sp)\n",
    "                if path_length < shortest_path_length:\n",
    "                    shortest_path_length, best_path = path_length, sp\n",
    "                    edges_in_best_path = [(sp[i], sp[i + 1]) for i in range(len(sp) - 1)]\n",
    "\n",
    "    # Store the results if a path was found\n",
    "    if shortest_path_length < float('inf'):\n",
    "        #print(f\"Shortest path between {node_a} and {node_b} is {shortest_path_length}\")\n",
    "        all_shortest_paths.append((node_a, node_b, shortest_path_length, best_path))\n",
    "\n",
    "        # Create subgraphs for GT \n",
    "        GT = G_carall.induced_subgraph([G_carall.vs[idx] for idx in best_path])\n",
    "        GTs.append(GT)\n",
    "    else:\n",
    "        print(f\"No path found between {node_a} and {node_b}\")\n",
    "\n",
    "# Output results\n",
    "print(\"Shortest paths for POI pairs:\", all_shortest_paths)\n",
    "print(\"Generated GTs:\", GTs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# CURENT BEST-or not ATTEMPT #############\n",
    "\n",
    "# Load graph and set CRS if not present\n",
    "G_carall = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'carall')\n",
    "if 'crs' not in G_carall.graph:\n",
    "    G_carall.graph['crs'] = 'epsg:4326'\n",
    "\n",
    "# Load neighbourhoods and convert graph to GeoDataFrames\n",
    "neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "exit_points = get_exit_nodes(neighbourhoods, G_carall)\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(placeid + \": Generating networks\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "\n",
    "\n",
    "    ###### neighbourhood speific code from here: ######\n",
    "    # Initialize list to store shortest paths for each POI pair\n",
    "    all_shortest_paths = []\n",
    "    GTs = []\n",
    "    GT_abstracts = []\n",
    "    processed_pairs = set()  # To track processed exit point pairs\n",
    "\n",
    "    # Iterate through each prune_quantile for greedy triangulation\n",
    "    for prune_quantile in tqdm(prune_quantiles, desc=\"Greedy triangulation\", leave=False):\n",
    "        # Load POIs\n",
    "        G = copy.deepcopy(G_carall)\n",
    "        pois = all_centroids['nearest_node'].tolist()\n",
    "        pois_indices = set()\n",
    "        for poi in pois:\n",
    "            pois_indices.add(G.vs.find(id = poi).index)\n",
    "        # Create a subgraph for the current prune_quantile\n",
    "        G_temp = copy.deepcopy(G)\n",
    "        for e in G_temp.es:\n",
    "            G_temp.es.delete(e)\n",
    "        poipairs = poipairs_by_distance(G, pois, weighting, True)\n",
    "        GT_abstract = copy.deepcopy(G_temp.subgraph(pois_indices))\n",
    "        GT_abstract = greedy_triangulation(G, poipairs, prune_quantile, prune_measure)\n",
    "        GT_abstracts.append(GT_abstract)\n",
    "\n",
    "        # Get node pairs we need to route, sorted by distance\n",
    "        routenodepairs = {}\n",
    "        for e in GT_abstract.es:\n",
    "            routenodepairs[(e.source_vertex[\"id\"], e.target_vertex[\"id\"])] = e[\"weight\"]\n",
    "        routenodepairs = sorted(routenodepairs.items(), key=lambda x: x[1])\n",
    "\n",
    "        # Do the routing\n",
    "        GT_indices = set()\n",
    "        for poipair, poipair_distance in routenodepairs:\n",
    "            poipair_ind = (G.vs.find(id = poipair[0]).index, G.vs.find(id = poipair[1]).index)\n",
    "            # debug\n",
    "            #print(f\"Edge weights before routing: {G.es['weight'][:10]}\")  # Prints first 10 weights\n",
    "            #print(f\"Routing between: {poipair[0]} and {poipair[1]} with distance: {poipair_distance}\")\n",
    "            sp = set(G.get_shortest_paths(poipair_ind[0], poipair_ind[1], weights = \"weight\", output = \"vpath\")[0])\n",
    "            #print(f\"Shortest path between {poipair[0]} and {poipair[1]}: {sp}\")\n",
    "\n",
    "            GT_indices = GT_indices.union(sp)\n",
    "            \n",
    "\n",
    "        ### THIS SECTION OF CODE IS FOR ROUTING BETWEEN EXIT POINTS ###\n",
    "        print(\"Beginning routing between exit points\")\n",
    "        # Get points based on neighbourhood centroids\n",
    "        pois = all_centroids['nearest_node'].tolist()\n",
    "        poipairs = poipairs_by_distance(G_carall, pois, weighting, return_distances=True)\n",
    "\n",
    "        # Track processed exit point pairs to avoid redundant routing\n",
    "        processed_pairs = set()\n",
    "\n",
    "        # Process each POI pair from poipairs\n",
    "        for (node_a, node_b), distance in poipairs:\n",
    "            print(\"Processing POI pair:\", node_a, node_b)\n",
    "            # Identify neighborhoods for each POI and retrieve their exit points\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == node_a, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == node_b, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points[exit_points['neighbourhood_id'] == neighbourhood_a].index\n",
    "            exit_points_b = exit_points[exit_points['neighbourhood_id'] == neighbourhood_b].index\n",
    "            shortest_path_length, best_path, edges_in_best_path = float('inf'), None, []\n",
    "            # Compute shortest paths between exit point pairs\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs: \n",
    "                        continue  # Skip if already processed\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    ea_vertex_index = G_carall.vs.find(id=ea).index\n",
    "                    eb_vertex_index = G_carall.vs.find(id=eb).index\n",
    "                    sp = G_carall.get_shortest_paths(ea_vertex_index, eb_vertex_index, weights=\"weight\", output=\"vpath\")[0]\n",
    "                    # Update if a shorter path is found\n",
    "                    if sp:\n",
    "                        path_length = len(sp)\n",
    "                        if path_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = path_length, sp\n",
    "                            edges_in_best_path = [(sp[i], sp[i + 1]) for i in range(len(sp) - 1)]\n",
    "\n",
    "            # Store the results if a path was found\n",
    "            if shortest_path_length < float('inf'):\n",
    "                #print(f\"Shortest path between {node_a} and {node_b} is {shortest_path_length}\")\n",
    "                all_shortest_paths.append((node_a, node_b, shortest_path_length, best_path))\n",
    "                # Create subgraphs for GT \n",
    "                GT = G_carall.induced_subgraph([G_carall.vs[idx] for idx in best_path])\n",
    "                GTs.append(GT)\n",
    "                print(\"Stored GT\")\n",
    "            else:\n",
    "                print(f\"No path found between {node_a} and {node_b}\")\n",
    "\n",
    "    # msts \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "                   \n",
    "    (MST, MST_abstract) = mst_routing(G_carall, nnids, weighting)\n",
    "\n",
    "    # Restore orignal edge lengths\n",
    "    if weighting == True:\n",
    "        restore_original_lengths(G_carall)\n",
    "        for GT in GTs:\n",
    "            restore_original_lengths(GT)\n",
    "        restore_original_lengths(MST)\n",
    "\n",
    "    # Write results\n",
    "    results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": prune_quantiles, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts, \"MST\": MST, \"MST_abstract\": MST_abstract}\n",
    "    write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(placeid + \": Generating networks\")\n",
    "\n",
    "    # Load networks\n",
    "    G_carall = csv_to_ig(PATH[\"data\"] + placeid + \"/\", placeid, 'carall', weighting=weighting)\n",
    "        \n",
    "    \n",
    "    # Load POIs\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + poi_source + '_nnidscarall.csv') as f:\n",
    "        nnids = [int(line.rstrip()) for line in f]\n",
    "    \n",
    "    # Generation\n",
    "    (GTs, GT_abstracts) = greedy_triangulation_routing(G_carall, nnids, weighting, prune_quantiles, prune_measure)\n",
    "    (MST, MST_abstract) = mst_routing(G_carall, nnids, weighting)\n",
    "    \n",
    "    # Restore orignal edge lengths\n",
    "    if weighting == True:\n",
    "        restore_original_lengths(G_carall)\n",
    "        for GT in GTs:\n",
    "            restore_original_lengths(GT)\n",
    "        restore_original_lengths(MST)\n",
    "\n",
    "\n",
    "    # Write results\n",
    "    results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": prune_quantiles, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts, \"MST\": MST, \"MST_abstract\": MST_abstract}\n",
    "    write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
